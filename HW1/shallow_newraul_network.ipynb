{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1 : MNIST reading & splitting\n",
    "ori_x, ori_y = loadlocal_mnist(\n",
    "    images_path='MNIST/train-images.idx3-ubyte', \n",
    "    labels_path='MNIST/train-labels.idx1-ubyte'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname='ori_images.csv', \n",
    "           X = ori_x, delimiter=',', fmt='%d')\n",
    "np.savetxt(fname='ori_labels.csv', \n",
    "           X = ori_y, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot encoding的編碼邏輯為將類別拆成多個行(column)，每個列中的數值由1、0替代，當某一列的資料存在的該行的類別則顯示1，反則顯示0。\n",
    "def oneHot(x, n_col=None):\n",
    "  \"\"\" One hot encoding function\"\"\"\n",
    "  if not n_col:\n",
    "    n_col = np.amax(x) + 1\n",
    "  \n",
    "  one_hot = np.zeros((x.shape[0], n_col))\n",
    "  one_hot[np.arange(x.shape[0]), x] = 1\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "  #小數點後兩位\n",
    "  return round(np.sum(y_true == y_pred, axis = 0) / len(y_true), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(X, y = None, batch_size=64):\n",
    "  \"\"\" Generates batches for training\"\"\"\n",
    "  n_samples = X.shape[0]\n",
    "  for i in np.arange(0, n_samples, batch_size):\n",
    "    begin, end = i, min(i + batch_size, n_samples)\n",
    "    if y is not None:\n",
    "      yield X[begin:end], y[begin: end]\n",
    "    else:\n",
    "      yield X[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 42000\n",
    "#70% train 30% test\n",
    "x_train, x_test = ori_x[:m], ori_x[m:]\n",
    "y_train, y_test = ori_y[:m], ori_y[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = oneHot(y_train.astype(\"int\")), oneHot(y_test.astype(\"int\"))\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (18000, 784))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = x_train.reshape(-1, 28*28), x_test.reshape(-1, 28*28)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28 # 784\n",
    "output_dim = 10 # 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2 : Dense neural layer:\n",
    "class Linear():\n",
    "  def __init__(self, input_dim, output_dim, name = \"linear\"):\n",
    "    limit = 1 / np.sqrt(input_dim)\n",
    "    self.W = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
    "    #initially set zero \n",
    "    self.b = np.zeros((1, output_dim)) \n",
    "    self.input = None\n",
    "    self.output = None\n",
    "    self.name = name\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.input = x\n",
    "    self.output = np.dot(self.input, self.W) + self.b \n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, output_error, learning_rate = 0.01):\n",
    "    input_error = np.dot(output_error, self.W.T)\n",
    "    # Calculate the weights error\n",
    "    delta = np.dot(self.input.T, output_error) \n",
    "\n",
    "    # 這裡使用sgd來更新參數\n",
    "    self.W -= learning_rate * delta\n",
    "    self.b -= learning_rate * np.mean(output_error)\n",
    "    return input_error\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3: ReLU layer:\n",
    "class ReLU():\n",
    "  def __init__(self, alpha = 0.2):\n",
    "    self.alpha = alpha\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    return self.activation(x)\n",
    "  # ReLU activation\n",
    "  def activation(self, x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "  \n",
    "  def gradient(self, x):\n",
    "    return np.where(x >= 0, 1, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4: Softmax output:\n",
    "class Softmax():\n",
    "  def __call__(self, x):\n",
    "    return self.activation(x)\n",
    "  \n",
    "  def activation(self, x):\n",
    "    e_x = np.exp(x - np.max(x, axis = -1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims = True)\n",
    "  \n",
    "  def gradient(self, x):\n",
    "    # Error was in our softmax\n",
    "    p = self.activation(x)\n",
    "    return p * (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step5: Cross-entropy loss calculation:\n",
    "class CrossEntropy():\n",
    "  def loss(self, y, p):\n",
    "    p = np.clip(p, 1e-15, 1- 1e-15)\n",
    "    return -y*np.log(p) - (1 - y) * np.log(1- p)\n",
    "  \n",
    "  def gradient(self, y, p):\n",
    "    p = np.clip(p, 1e-15, 1- 1e-15)\n",
    "    return -(y/p) + (1 - y) / (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "  def __init__(self, activation, name = \"activation\"):\n",
    "    self.activation = activation\n",
    "    self.gradient = activation.gradient\n",
    "    self.input = None\n",
    "    self.output = None\n",
    "    self.name = name\n",
    "  #calculate the output\n",
    "  def forward(self, x):\n",
    "    self.input = x\n",
    "    self.output = self.activation(x)\n",
    "    return self.output\n",
    "  #backward to update the weight and biase\n",
    "  def backward(self, output_error, learning_rate = 0.01):\n",
    "    return self.gradient(self.input) * output_error\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "  def __init__(self, input_dim, output_dim, learning_rate = 0.01):\n",
    "    # input_dim = 784, output_dim = 10 for mnist\n",
    "    self.layers = [\n",
    "                   Linear(input_dim, 512, name = \"input\"),\n",
    "                   Activation(ReLU(), name = \"hidden_1\"),\n",
    "                   Linear(512, 256, name = \"input\"),\n",
    "                   Activation(ReLU(), name = \"hidden_2\"),\n",
    "                   Linear(256, output_dim, name = \"output\"),\n",
    "                   Activation(Softmax(), name = \"softmax\")\n",
    "    ]\n",
    "    self.learning_rate = learning_rate\n",
    "  \n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "  #step6: Backward propagation:\n",
    "  def backward(self, loss_grad):\n",
    "    for layer in reversed(self.layers):\n",
    "      loss_grad = layer.backward(loss_grad, self.learning_rate)\n",
    "    # Iterating backwards through the layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropy()\n",
    "model = Network(input_dim, output_dim, learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, ValidationLoss: 0.055263398175666896, ValidationAcc: 0.9000709219858156, TrainingLoss: 0.10845611887307162, TrainingAcc: 0.830289193302892\n",
      "Epoch 2, ValidationLoss: 0.04188900874879076, ValidationAcc: 0.9276241134751774, TrainingLoss: 0.04649470391894461, TrainingAcc: 0.9213394216133941\n",
      "Epoch 3, ValidationLoss: 0.0338745065336244, ValidationAcc: 0.9418085106382977, TrainingLoss: 0.035096432778106076, TrainingAcc: 0.9412024353120242\n",
      "Epoch 4, ValidationLoss: 0.028947008455809338, ValidationAcc: 0.9506382978723403, TrainingLoss: 0.028024720328670965, TrainingAcc: 0.9521613394216133\n",
      "Epoch 5, ValidationLoss: 0.025676270176351434, ValidationAcc: 0.9553900709219859, TrainingLoss: 0.023163417672677947, TrainingAcc: 0.9611111111111111\n",
      "Epoch 6, ValidationLoss: 0.023377193860218966, ValidationAcc: 0.9588297872340426, TrainingLoss: 0.019556071368199077, TrainingAcc: 0.9677168949771688\n",
      "Epoch 7, ValidationLoss: 0.021705432309929, ValidationAcc: 0.9620567375886523, TrainingLoss: 0.01672413399538133, TrainingAcc: 0.9726179604261795\n",
      "Epoch 8, ValidationLoss: 0.02043548089484053, ValidationAcc: 0.9645035460992908, TrainingLoss: 0.01440716216746476, TrainingAcc: 0.9766210045662101\n",
      "Epoch 9, ValidationLoss: 0.019535580748808325, ValidationAcc: 0.9659574468085106, TrainingLoss: 0.01247885402399946, TrainingAcc: 0.9798630136986302\n"
     ]
    }
   ],
   "source": [
    "# x is image y is label\n",
    "# 收集loss data作圖用\n",
    "plot_loss = []\n",
    "plot_validate = []\n",
    "epoch_iter = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  loss = []\n",
    "  acc = []\n",
    "  test_loss = []\n",
    "  test_acc = []\n",
    "\n",
    "  for x_batch, y_batch in batch_loader(x_train, y_train):\n",
    "    out = model(x_batch) # Forward pass\n",
    "    loss.append(np.mean(criterion.loss(y_batch, out))) \n",
    "    # We just passed the inputs incorrectly.\n",
    "    acc.append(accuracy(np.argmax(y_batch, axis=1), np.argmax(out, axis=1))) \n",
    "    error = criterion.gradient(y_batch, out) # Calculate gradient of loss\n",
    "    #step6: Backpropagation\n",
    "    model.backward(error) \n",
    "    #step7: Validation\n",
    "  for x_batch, y_batch in batch_loader(x_test, y_test):\n",
    "    #step8: Testing accuracy\n",
    "    test_out = model(x_batch)\n",
    "    test_loss.append(np.mean(criterion.loss(y_batch, test_out))) \n",
    "    test_acc.append(accuracy(np.argmax(y_batch, axis=1), np.argmax(test_out, axis=1))) \n",
    "  plot_loss.append(np.mean(loss))\n",
    "  plot_validate.append(np.mean(test_loss))\n",
    "  epoch_iter.append(epoch + 1)\n",
    "\n",
    "  print(f\"Epoch {epoch + 1}, ValidationLoss: {np.mean(test_loss)}, ValidationAcc: {np.mean(test_acc)}, TrainingLoss: {np.mean(loss)}, TrainingAcc: {np.mean(acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predic = model(x_test) \n",
    "accuracy(np.argmax(y_test, axis=1), np.argmax(y_predic, axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Result Analysis')\n",
    "plt.plot(epoch_iter, plot_loss, color='green', label='training accuracy')\n",
    "plt.plot(epoch_iter, plot_validate, color='red', label='testing accuracy')\n",
    "plt.xlabel('iteration times')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
