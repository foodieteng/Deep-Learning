{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow_addons as tfa\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from datetime import date, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "path = ''\n",
    "# path = 'data 2'\n",
    "num_classes = 11\n",
    "input_dim = 31\n",
    "train_epochs = 6200\n",
    "tf.random.set_seed(1000)\n",
    "\n",
    "\n",
    "def read_datasets(path):\n",
    "    with open(path, newline='') as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            data.append(row)\n",
    "        print('load success from', path)\n",
    "        return data[1:]\n",
    "\n",
    "\n",
    "def split_data(train_data):\n",
    "    x_train = train_data\n",
    "    y_train = []\n",
    "    print(len(train_data))\n",
    "    for row in x_train:\n",
    "        result = row.pop()\n",
    "        y_train.append(result)\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def process_data(x_train, x_test):\n",
    "    new_x_train = []\n",
    "    for data in x_train:\n",
    "        new_data = []\n",
    "        data = [data[1]]+data[4:]\n",
    "        for value in data:\n",
    "            new_data.append(float(value))\n",
    "        new_x_train.append(new_data)\n",
    "\n",
    "    new_x_test = []\n",
    "    for data in x_test:\n",
    "        new_data = []\n",
    "        data_ = [data[1]]+data[4:]\n",
    "        for value in data_:\n",
    "            new_data.append(float(value))\n",
    "        new_data = new_data[:input_dim]\n",
    "        new_x_test.append(new_data)\n",
    "\n",
    "    return new_x_train, new_x_test\n",
    "\n",
    "def predict(x_test, model):\n",
    "    y_test = []\n",
    "    predict_results = model.predict(x_test)\n",
    "    for result in predict_results:\n",
    "        y_test.append(np.argmax(result))\n",
    "    return y_test\n",
    "\n",
    "def train_model_and_predict(x_train, x_test, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=128, input_dim=input_dim,\n",
    "              kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=64, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    \n",
    "\n",
    "    model.add(Dense(units=num_classes,\n",
    "              kernel_initializer='normal', activation='softmax'))\n",
    "    kap = tfa.metrics.CohenKappa(num_classes=num_classes, sparse_labels=False)\n",
    "    adam = tensorflow.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam, metrics=['accuracy', kap])\n",
    "    train_history = model.fit(x=x_train, y=y_train, validation_split=0.1,\n",
    "                              epochs=train_epochs, batch_size=100, verbose=1)\n",
    "\n",
    "    y_test = predict(x_test, model)\n",
    "    return y_test, model, train_history\n",
    "\n",
    "\n",
    "def plot(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    kap = history.history['cohen_kappa']\n",
    "    val_kap = history.history['val_cohen_kappa']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(train_epochs)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range, kap, label='Training Kappa')\n",
    "    plt.plot(epochs_range, val_kap, label='Validation Kappa')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Kappa')\n",
    "    plt.savefig('Result')\n",
    "    plt.show()\n",
    "\n",
    "def print_kappa_matrix(y_train, y_pred):\n",
    "    yt = tf.reshape(tf.cast(K.round(y_train), dtype=tf.int32), shape=(319,))\n",
    "    yp = tf.reshape(K.round(y_pred), shape=(319,))\n",
    "    akap = tfa.metrics.CohenKappa(num_classes=num_classes, sparse_labels=True)\n",
    "    a = akap.update_state(yt, yp)\n",
    "    print(f'Kappa Value = {float(akap.result())}')\n",
    "    print('Kappa Matrix')\n",
    "    print(np.array(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "load success from preprocess_train拷貝.csv\n",
      "319\n",
      "load success from preprocess_test拷貝.csv\n"
     ]
    }
   ],
   "source": [
    "print('Loading datasets...')\n",
    "train_data = read_datasets('preprocess_train拷貝.csv')\n",
    "x_train, y_train_ = split_data(train_data)\n",
    "test_data = read_datasets('preprocess_test拷貝.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test_data, []\n",
    "\n",
    "print(len(x_train), len(x_test), len(y_train_), len(y_test))\n",
    "x_train, x_test = process_data(x_train, x_test)\n",
    "\n",
    "print('Reshaping data...')\n",
    "x_train = np.array(x_train, dtype=float)\n",
    "print('x_train', x_train.shape)\n",
    "x_test = np.array(x_test, dtype=float)\n",
    "print('x_test', x_test.shape)\n",
    "y_train_ = np.array(y_train_, dtype=int)\n",
    "print('y_train', y_train_.shape)\n",
    "\n",
    "y_train = to_categorical(y_train_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"concavity_mean\"]=((data[\"concavity_mean\"]-data[\"concavity_mean\"].min())/(data[\"concavity_mean\"].max()-data[\"concavity_mean\"].min()))*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "scaled_x = NormalizeData(x_train)\n",
    "\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data...')\n",
    "y_test, model, history = train_model_and_predict(x_train, x_test, y_train)\n",
    "y_pred = predict(x_train, model)\n",
    "plot(history)\n",
    "print_kappa_matrix(y_train_, y_pred)\n",
    "print('Predict_result: ', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import numpy as np\n",
    "# import tensorflow.keras\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Activation\n",
    "# import tensorflow_addons as tfa\n",
    "# #from tensorflow.keras.utils import np_utils\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from datetime import date, datetime\n",
    "\n",
    "# num_classes = 11\n",
    "# input_dim = 30\n",
    "# train_epochs = 175\n",
    "\n",
    "\n",
    "# def read_datasets(path):\n",
    "#     with open(path, newline='') as csvfile:\n",
    "#         rows = csv.reader(csvfile)\n",
    "#         data = []\n",
    "#         for row in rows:\n",
    "#             data.append(row)\n",
    "#         print('load success from', path)\n",
    "#         return data[1:]\n",
    "\n",
    "# def print_kappa_matrix(y_train, y_pred):\n",
    "#     yt = tf.reshape(tf.cast(K.round(y_train), dtype=tf.int32), shape=(319,))\n",
    "#     yp = tf.reshape(K.round(y_pred), shape=(319,))\n",
    "#     akap = tfa.metrics.CohenKappa(num_classes=num_classes, sparse_labels=True)\n",
    "#     a = akap.update_state(yt, yp)\n",
    "#     print(f'Kappa Value = {float(akap.result())}')\n",
    "#     print('Kappa Matrix')\n",
    "#     print(np.array(a))\n",
    "\n",
    "\n",
    "# def split_data(train_data):\n",
    "#     x_train = train_data\n",
    "#     y_train = []\n",
    "#     print(len(train_data))\n",
    "#     for row in x_train:\n",
    "#         result = row.pop()\n",
    "#         y_train.append(result)\n",
    "#     return x_train, y_train\n",
    "\n",
    "\n",
    "# def process_data(x_train, x_test):\n",
    "#     new_x_train = []\n",
    "#     for data in x_train:\n",
    "#         new_data = []\n",
    "#         data = [data[1]]+data[4:]\n",
    "#         for value in data:\n",
    "#             new_data.append(float(value))\n",
    "#         new_x_train.append(new_data)\n",
    "\n",
    "#     new_x_test = []\n",
    "#     for data in x_test:\n",
    "#         new_data = []\n",
    "#         data = [data[1]]+data[4:]\n",
    "#         for value in data_:    \n",
    "#             new_data.append(float(value))\n",
    "#         new_data = new_data[:30]\n",
    "#         new_x_test.append(new_data)\n",
    "\n",
    "#     return new_x_train, new_x_test\n",
    "\n",
    "\n",
    "# def train_model_and_predict(x_train, x_test, y_train):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=256, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(units=256, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(units=256, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "#     model.add(Dense(units=num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "#     kap = tfa.metrics.CohenKappa(num_classes=num_classes, sparse_labels=False)\n",
    "#     #adam = tensorflow.keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     train_history = model.fit(x=x_train, y=y_train, validation_split=0.2, epochs=train_epochs, batch_size=32, verbose=1)\n",
    "\n",
    "#     y_test = []\n",
    "#     predict_results = model.predict(x_test)\n",
    "#     for result in predict_results:\n",
    "#         y_test.append(np.argmax(result))\n",
    "#     return y_test, model\n",
    "\n",
    "\n",
    "# def write_submission_file(result):\n",
    "#     submission_rows = []\n",
    "#     with open('./data 2/submission.csv', newline='') as csvfile:\n",
    "#         rows = csv.reader(csvfile)\n",
    "#         for row in rows:\n",
    "#             submission_rows.append(row)\n",
    "\n",
    "#     for i in range(len(result)):\n",
    "#         submission_rows[i+1][1] = result[i]\n",
    "\n",
    "#     now = datetime.now()\n",
    "#     path = './submission_' + now.strftime(\"%Y-%m-%d_%H:%M:%S\") + '.csv'\n",
    "#     file = open(path, 'w')\n",
    "#     writer = csv.writer(file)\n",
    "#     for row in submission_rows:\n",
    "#         writer.writerow(row)\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# print('Loading datasets...')\n",
    "# train_data = read_datasets('preprocess_train.csv')\n",
    "# x_train, y_train = split_data(train_data)\n",
    "# test_data = read_datasets('preprocess_test.csv')\n",
    "# x_test, y_test = test_data, []\n",
    "\n",
    "# print(len(x_train), len(x_test), len(y_train), len(y_test))\n",
    "# x_train, x_test = process_data(x_train, x_test)\n",
    "\n",
    "# print('Reshaping data...')\n",
    "# x_train = np.array(x_train, dtype=float)\n",
    "# print('x_train', x_train.shape)\n",
    "# x_test = np.array(x_test, dtype=float)\n",
    "# print('x_test', x_test.shape)\n",
    "# y_train = np.array(y_train, dtype=int)\n",
    "# print('y_train', y_train.shape)\n",
    "\n",
    "# y_train = to_categorical(y_train, num_classes)\n",
    "\n",
    "# x_train[:,1] /= np.max(np.abs(x_train[:,1]))\n",
    "# x_test[:,1] /= np.max(np.abs(x_test[:,1]))\n",
    "\n",
    "# print('Training data...')\n",
    "# y_test, model = train_model_and_predict(x_train, x_test, y_train)\n",
    "# print('Predict_result: ', y_test)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predict_result: ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 CSV File\n",
    "import pandas as pd # 引用套件並縮寫為 pd  \n",
    "output = pd.read_csv('data2/submission.csv')  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len):\n",
    "    output['LEVEL'][i] = y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
